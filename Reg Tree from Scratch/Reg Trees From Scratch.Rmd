---
title: "Regression Trees from Scratch"
author: "Kevin Patyk"
date: "11/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction 

Most probably you are already familiar with decision trees, which is a machine learning algorithm to solve classification tasks. As the name itself states regression trees solve regressions, i.e. estimation with continuous scaled targets. These kind of trees are the key part of every tree-based method, since the way you grow a tree is more of the same really. The differing parts among the implementations are mostly about the splitting rule. In this tutorial we will program a very simple, but generic implementation of a regression tree.

Anyway, as most of you might know decision or regression trees are rule-based approaches. Meaning, we are trying to split the data into partitions conditional to our feature space. The data partitioning is done with the help of a splitting criterion. There is no common ground on how to do those splits, there are rather multiple different splitting criteria with different pros and cons. We will focus on a rather simple criterion in this tutorial. Bear with me, here comes some maths.

$$ SSE = \Sigma_{R1}(y_i - \bar{y_1})^2 + \Sigma_{R2}(y_i - \bar{y_2})^2 $$
Where $\bar{y_1}$ and $\bar{y_2}$ are the average values of the dependent variable in $R_1$ and $R_2$. 

Ok, so what does this state? This is the sum of squared errors determined in two different subsets ($R_1$ and $R_2$). As the name suggests, that should be something we want to minimize. In fact, it is the squared distance between the mean and the target within this data subset. In every node of our regression tree we calculate the SSE for every potential split we could do in our data for every feature we have to figure out the best split we can achieve.

Let us have a look at the R Code for the SSE.
```{r}
# This is the splitting criterion we minimize (SSE [Sum Of Squared Errors]):
# SSE = \sum{i \in S_1} (y_i - \bar(y)_1)^2 + \sum{i \in S_2} (y_i - \bar(y)_2)^2

sse_var <- function(x, y) { #this function will take x, the feature, and y, the outcome, as inputs 
  
  splits <- sort(unique(x)) #first, we are going to sort based on the unique values of x. So, this will take all of the unique values of x (removing duplicates) and put them in order from lowest to highest. These will serve as our splits. 
  
  sse <- c() #now, we are creating a storage vector for the for loop 
  
  #now, we are creating the for loop to iterate over the indices of the splits vector, which contains unique values from lowest to highest. This for loop will iterate the same number of times as there are indices in the splits vector that we have made.
  for (i in seq_along(splits)) {
    
    sp <- splits[i] #we are going to store the index that the current iteration is at in a variable called sp. This will consider every possible split such as (x < 4.3) & (x >= 4.3), then (x < 4.4) & (x >= 4.4) etc. until all splits have been accounted for 
    
    #now, we are going to calculate the sum of squared errors for that particular value of sp. We will do this using the SSE formula described above. We are going to calculate it for the first region using the y values that fall in (x < sp) and the second region using y values that fall in (x >= sp). We are going to store the sse into the storage vector we made previously. We will be filtering our y variable based on which values in the x variable meet our condition 
    sse[i] <- sum((y[x < sp] - mean(y[x < sp]))^2) +  
              sum((y[x >= sp] - mean(y[x >= sp]))^2) 
  } 
  
  #now, we are going to find the index that has the lowest sse 
  split_at <- splits[which.min(sse)]
  
  #we are going to return a vector that contains the lowest mse value and the index at which we should split 
  return(c(sse = min(sse), split = split_at))
}
```

The function takes two inputs our numeric feature `x` and our target real-valued `y`. We then go ahead and calculate the SSE for every unique value of our `x`. This means we calculate the SSE for every possible data subset we could obtain conditional on the feature. Often we want to cover more than one feature in our problem, which means that we have to run this function for every feature. As a result, the best splitting rule has the lowest SSE among all possible splits of all features. Once we have determined the best splitting rule, we can split our data into these two subsets according to our criterion, which is nothing else than feature `x <= split_at` and `x > split_at`. We call these two subsets nodes and they again can be split into subsets again.

Testing to make sure the function works.
```{r}
sse_var(x = iris$Sepal.Length, y = iris$Sepal.Width)
```

Let us lose some more words on the SSE though, because it reveals our estimator. In this implementation our estimator in the leaf is simply the average value of our target within this data subset. So, if x <= 4.5, then our predicted value will be the average of all x values <= 4.5 or if x > 4.5, then our predicted value will be the average of all x values > 4.5. This is the simplest version of a regression tree. However, with some additional work you can apply more sophisticated models, e.g. an ordinary least squares fit.

-----

# The Algorithm

Enough with the talking, let's get to the juice. In the following, we will breakdown the algorithm in easy-to-digest code chunks.

Let's have a look at the first code chunk.
```{r eval=FALSE}
  # coerce the data into the form of a data.frame
  data <- as.data.frame(data)
  
  # constructing a terms object from a formula
  formula <- terms.formula(formula)
  
  # getting a design matrix. In statistics and in particular in regression analysis, a design matrix, also known as model matrix or regressor matrix and often denoted by X, is a matrix of values of explanatory variables of a set of objects. Each row represents an individual observation, with the successive columns corresponding to the variables and their specific values for that observation. The design matrix is used in certain statistical models, e.g., the general linear model.It can contain indicator variables (ones and zeros) that indicate group membership in an ANOVA, or it can contain values of continuous variables.
  X <- model.matrix(formula, data)
  
  # extracting the outcome formula from the formula that we specified 
  y <- data[, as.character(formula)[2]]
  
  # initialize while loop
  do_splits <- TRUE
  
  # creating an output data.frame with splitting rules and observations
  tree_info <- data.frame(NODE = 1, NOBS = nrow(data), FILTER = NA, 
                          TERMINAL = "SPLIT",
                          stringsAsFactors = FALSE)
```

Essentially, this is everything before the `while` loop. Here, we see some input handling in the beginning and the extraction of our design matrix `X` and our target `y`. All our features are within this design matrix. `do_splits` is the condition for our while loop, which we will cover in a bit. The data frame `tree_info` is the key storage element in our algorithm, because it will contain every information we need about our tree. The essential piece of this object is the `filter` column. This column saves the paths (`filter`) we have to take through (to apply to) our data to get to a leaf (a terminal node) in our regression tree. We initiate this `data.frame` with `NODE = 1`, since at the beginning we are in the root node of our tree with the whole data set at our expense. Furthermore, there is a column called `TERMINAL`, which controls the state of the node. We have three different states `SPLIT`, `LEAF` and `PARENT`. When we describe a node with the `SPLIT` state, we mark it for a potential split. The state `PARENT` indicates, that we have already split this node. Lastly, the state `LEAF` marks terminal nodes of our regression tree.

## Reaching the tree top

When do we reach such a terminal node? In many implementations there is a minimum size parameter, where we determine valid splits by the amount of observations of its children. If the children have lower data points than the minimum size the split is invalid and will not be done. Imagine not having this parameter in our case, we could end up with leafs covering only one observation. Another termination rule is if the lowest SSE is at a split we have already invoked within the branch and hence has already been invoked in this branch. The split at such a point would be nonsense, since we would end up with the same subset over and over again.

Basically, this is everything there is to the algorithm. The `while` and `for` loop just ensure, that we estimate and create every node in our `tree_info`. Thus, you can perceive the `tree_info` as sort of a job list, since we create new jobs within this data frame. Let us walk through the actual R code.
```{r eval=FALSE}
 # keep splitting until there are only leafs left
  while(do_splits) {
    
    # which parents have to be split
    to_calculate <- which(tree_info$TERMINAL == "SPLIT")
    
    for (j in to_calculate) {
      
      # handle root node
      if (!is.na(tree_info[j, "FILTER"])) {
        # subset data according to the filter
        this_data <- subset(data, eval(parse(text = tree_info[j, "FILTER"])))
        # get the design matrix
        X <- model.matrix(formula, this_data)
      } else {
        this_data <- data
      }
      
      # estimate splitting criteria
      splitting <- apply(X,  MARGIN = 2, FUN = sse_var, y = y)
      
      # get the min SSE
      tmp_splitter <- which.min(splitting[1,])
      
      # define maxnode
      mn <- max(tree_info$NODE)
      
      # paste filter rules
      tmp_filter <- c(paste(names(tmp_splitter), ">=", 
                            splitting[2,tmp_splitter]),
                      paste(names(tmp_splitter), "<", 
                            splitting[2,tmp_splitter]))
      
      # Error handling! check if the splitting rule has already been invoked
      split_here  <- !sapply(tmp_filter,
                             FUN = function(x,y) any(grepl(x, x = y)),
                             y = tree_info$FILTER)
      
      # append the splitting rules
      if (!is.na(tree_info[j, "FILTER"])) {
        tmp_filter  <- paste(tree_info[j, "FILTER"], 
                             tmp_filter, sep = " & ")
      } 
      
      # get the number of observations in current node
      tmp_nobs <- sapply(tmp_filter,
                         FUN = function(i, x) {
                           nrow(subset(x = x, subset = eval(parse(text = i))))
                         },
                         x = this_data)  
      
      # insufficient minsize for split
      if (any(tmp_nobs <= minsize)) {
        split_here <- rep(FALSE, 2)
      }
      
      # create children data frame
      children <- data.frame(NODE = c(mn+1, mn+2),
                             NOBS = tmp_nobs,
                             FILTER = tmp_filter,
                             TERMINAL = rep("SPLIT", 2),
                             row.names = NULL)[split_here,]
      
      # overwrite state of current node
      tree_info[j, "TERMINAL"] <- ifelse(all(!split_here), "LEAF", "PARENT")
       
      # bind everything
      tree_info <- rbind(tree_info, children)
      
      # check if there are any open splits left
      do_splits <- !all(tree_info$TERMINAL != "SPLIT")
    } # end for
  } # end while
```

The `while` loop covers our tree depth and our `for` loop all splits within this certain depth. Within the `while` loop we seek every row in our `tree_info`, which we still have to estimate, i.e. all nodes in the `SPLIT` state. In the first iteration it would be the first row, our root node. The `for` loop iterates over all potential splitting nodes. The first `if` condition ensures that we filter our data according to the tree depth. Of course, there is no filter in the root node that is why we take the data as is. But imagine calculating possible splits for a parent in depth level two. The filter would look similar to this one: `feature_1 > 5.33` & `feature_2 <= 3.22`.

## How do we apply splitting?

Afterwards we seek the minimum SSE by applying the `sse_var` function to every feature. Note, that in this version we can only handle numeric features. Once we have found the best splitting variable, here named `tmp_splitter`, we build the according filter rule in object `tmp_filter`.

We still have to check if this is a valid split, i.e. we have not invoked this split for this branch yet and we have sufficient observations in our children. Our indicator `split_here` rules whether we split our node. Well, that is about it. In the last passage of the loop we prepare the output for this node. That is, handling the state of the calculated node and adding the children information to our job list `tree_info`. After the `for` loop we have to check whether our tree is fully grown. The variable `do_splits` checks whether there are any nodes left we have to calculate. We terminate the calculation if there are no `SPLIT` nodes left in our `tree_info`.
```{r eval=FALSE}
 # calculate fitted values
  leafs <- tree_info[tree_info$TERMINAL == "LEAF", ]
  fitted <- c()
  for (i in seq_len(nrow(leafs))) {
    # extract index
    ind <- as.numeric(rownames(subset(data, eval(parse(
        					   text = leafs[i, "FILTER"])))))
    # estimator is the mean y value of the leaf
    fitted[ind] <- mean(y[ind])
  }
```

At the end of our calculation we have a filter rule for every leaf in our tree. With the help of these filters we can easily calculate the fitted values by simply applying the filter on our data and calculating our fit, i.e. the mean of our target in this leaf. I am sure by now you can think of a way to implement more sophisticated estimators, which I would leave up to you.

-----

# The Entire Algorithm
```{r}
reg_tree <- function(formula, data, minsize) {
  
  # coerce to data.frame
  data <- as.data.frame(data)
  
  # handle formula
  formula <- terms.formula(formula)
  
  # get the design matrix
  X <- model.matrix(formula, data)
  
  # extract target
  y <- data[, as.character(formula)[2]]
  
  # initialize while loop
  do_splits <- TRUE
  
  # create output data.frame with splitting rules and observations
  tree_info <- data.frame(NODE = 1, NOBS = nrow(data), FILTER = NA,
                          TERMINAL = "SPLIT",
                          stringsAsFactors = FALSE)
  
  # keep splitting until there are only leafs left
  while(do_splits) {
    
    # which parents have to be splitted
    to_calculate <- which(tree_info$TERMINAL == "SPLIT")
    
    for (j in to_calculate) {
      
      # handle root node
      if (!is.na(tree_info[j, "FILTER"])) {
        # subset data according to the filter
        this_data <- subset(data, eval(parse(text = tree_info[j, "FILTER"])))
        # get the design matrix
        X <- model.matrix(formula, this_data)
      } else {
        this_data <- data
      }
      
      # estimate splitting criteria
      splitting <- apply(X,  MARGIN = 2, FUN = sse_var, y = y)
      
      # get the min SSE
      tmp_splitter <- which.min(splitting[1,])
      
      # define maxnode
      mn <- max(tree_info$NODE)
      
      # paste filter rules
      tmp_filter <- c(paste(names(tmp_splitter), ">=", 
                            splitting[2,tmp_splitter]),
                      paste(names(tmp_splitter), "<", 
                            splitting[2,tmp_splitter]))
      
      # Error handling! check if the splitting rule has already been invoked
      split_here  <- !sapply(tmp_filter,
                             FUN = function(x,y) any(grepl(x, x = y)),
                             y = tree_info$FILTER)
      
      # append the splitting rules
      if (!is.na(tree_info[j, "FILTER"])) {
        tmp_filter  <- paste(tree_info[j, "FILTER"], 
                             tmp_filter, sep = " & ")
      } 
      
      # get the number of observations in current node
      tmp_nobs <- sapply(tmp_filter,
                         FUN = function(i, x) {
                           nrow(subset(x = x, subset = eval(parse(text = i))))
                         },
                         x = this_data)  
      
      # insufficient minsize for split
      if (any(tmp_nobs <= minsize)) {
        split_here <- rep(FALSE, 2)
      }
      
      # create children data frame
      children <- data.frame(NODE = c(mn+1, mn+2),
                             NOBS = tmp_nobs,
                             FILTER = tmp_filter,
                             TERMINAL = rep("SPLIT", 2),
                             row.names = NULL)[split_here,]
      
      # overwrite state of current node
      tree_info[j, "TERMINAL"] <- ifelse(all(!split_here), "LEAF", "PARENT")
       
      # bind everything
      tree_info <- rbind(tree_info, children)
      
      # check if there are any open splits left
      do_splits <- !all(tree_info$TERMINAL != "SPLIT")
    } # end for
  } # end while
  
  # calculate fitted values
  leafs <- tree_info[tree_info$TERMINAL == "LEAF", ]
  fitted <- c()
  for (i in seq_len(nrow(leafs))) {
    # extract index
    ind <- as.numeric(rownames(subset(data, eval(parse(text = leafs[i, "FILTER"])))))
    # estimator is the mean y value of the leaf
    fitted[ind] <- mean(y[ind])
  }
  
  # return everything
  return(list(tree = tree_info, fit = fitted, formula = formula, data = data))
}
```

Checking to make sure that it works.
```{r}
reg_tree(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, data = iris[, -5], minsize = 5)
```

-----

# End of document

-----

```{r}
sessionInfo()
```

